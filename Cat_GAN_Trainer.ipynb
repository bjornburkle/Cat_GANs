{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU6PNqtQB91V"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from IPython import display\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum about of VRAM that TF is eating\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the use of mixed precision\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dV1AkxQxbTO"
   },
   "outputs": [],
   "source": [
    "# Set of training set\n",
    "\n",
    "train_dir = os.path.join('.', 'big_imageset/')\n",
    "train_cats_dir = os.path.join(train_dir, 'Cat/')\n",
    "\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "print(train_cat_fnames[:10])\n",
    "\n",
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "ncats = len(os.listdir(train_cats_dir))\n",
    "print(ncats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 3783,
     "status": "ok",
     "timestamp": 1626804969709,
     "user": {
      "displayName": "Bjorn Burkle",
      "photoUrl": "",
      "userId": "16320588017453975734"
     },
     "user_tz": 240
    },
    "id": "A5cLMyjcyhtM",
    "outputId": "312909bd-2f2b-4a3c-f82e-b3e5e710ea55",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Draw some cats\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18261,
     "status": "ok",
     "timestamp": 1626805014090,
     "user": {
      "displayName": "Bjorn Burkle",
      "photoUrl": "",
      "userId": "16320588017453975734"
     },
     "user_tz": 240
    },
    "id": "tQz2gDGVHHCw",
    "outputId": "e3627c80-8a47-49f7-d2f5-1fa3cc36023d"
   },
   "outputs": [],
   "source": [
    "# Setup dataset\n",
    "\n",
    "BATCH_SIZE=64\n",
    "CHECKPOINT_FREQ=50\n",
    "\n",
    "cat_labels = np.array([1. for _ in range(ncats)])\n",
    "cat_generator = train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./127.5)\n",
    "train_images = cat_generator.flow_from_directory(\n",
    "    train_dir,\n",
    "    color_mode='rgb',\n",
    "    class_mode=None,\n",
    "    target_size=(128,128),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "batch_per_epoch = ncats//BATCH_SIZE\n",
    "print('Total number of batches:', batch_per_epoch)\n",
    "\n",
    "print('image shape:', train_images[0][0,...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1626805056678,
     "user": {
      "displayName": "Bjorn Burkle",
      "photoUrl": "",
      "userId": "16320588017453975734"
     },
     "user_tz": 240
    },
    "id": "b-8ByGgt5zWQ",
    "outputId": "a3ac4f3c-f89f-4962-8c78-64c97707f14a"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Plotting an image')\n",
    "#print(np.array(train_images[5][1,...]))\n",
    "#plt.imshow(np.array(train_images[0][0,...,0]), interpolation='none', cmap='gray')\n",
    "plt.imshow((np.array(train_images[7][5,...])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBMfJK1oNid9"
   },
   "outputs": [],
   "source": [
    "# Make generator using a sequential model\n",
    "\n",
    "def make_generator_model(input_shape=(200,)):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    x = tf.keras.layers.Dense(4*4*2048, use_bias=False, input_shape=input_shape)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Reshape((4,4,2048))(x)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    # 4 -> 8\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(1024, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    print(x.shape)\n",
    "\n",
    "    # 8 -> 16\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(512, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    # 16 -> 32\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(256, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    # 32 -> 64\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    print(x.shape)\n",
    "    \n",
    "    #64 -> 128\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    #128 -> 128\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = tf.keras.layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    #x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    #print(x.shape)\n",
    "    \n",
    "    #128 -> 128\n",
    "    x = tf.keras.layers.Conv2D(3, (3,3), strides=(1,1), padding='same', use_bias=False, activation='tanh')(x)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=\"generator\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1rc_l4XF7Gd"
   },
   "outputs": [],
   "source": [
    "# Make discriminator model\n",
    "\n",
    "def make_discriminator_model(input_shape = (128, 128, 3)):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    print(inputs.shape)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, (4, 4), strides=(1, 1), padding='same')(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3,3), strides=(2,2), padding='same')(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(1024, (2,2), strides=(2,2), padding='same')(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(2048, (2,2), strides=(2,2), padding='same')(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    print(x.shape)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    print(x.shape)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36356,
     "status": "ok",
     "timestamp": 1626805110336,
     "user": {
      "displayName": "Bjorn Burkle",
      "photoUrl": "",
      "userId": "16320588017453975734"
     },
     "user_tz": 240
    },
    "id": "55hpLsXwHvIz",
    "outputId": "01c83c21-bf1a-49df-9bd4-811cefe0ea05"
   },
   "outputs": [],
   "source": [
    "# create the two models\n",
    "\n",
    "noise = tf.random.normal([1,50])\n",
    "#print(noise)\n",
    "\n",
    "generator = make_generator_model((noise.shape[1],))\n",
    "generated_image = generator(noise, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhCFWEhNDwlO"
   },
   "outputs": [],
   "source": [
    "# plot the image\n",
    "print(generated_image.shape)\n",
    "\n",
    "Z = (np.array(generated_image[0,...], dtype=np.float32)+1)/2\n",
    "#print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1626805127182,
     "user": {
      "displayName": "Bjorn Burkle",
      "photoUrl": "",
      "userId": "16320588017453975734"
     },
     "user_tz": 240
    },
    "id": "DOGiNUvMInNq",
    "outputId": "b43141d5-f0e3-47ce-c864-dfa2e9d81217"
   },
   "outputs": [],
   "source": [
    "# make the discriminator\n",
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XMMIwZTLmKB"
   },
   "outputs": [],
   "source": [
    "# define the loss functions for the GAN\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(real), real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake), fake)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return cross_entropy(tf.ones_like(fake), fake)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY1YOGRFMxhu"
   },
   "outputs": [],
   "source": [
    "# make place to save models and checkpoints\n",
    "\n",
    "\n",
    "save_dir = './cat_training_checkpoints_x128-extra-dense-out'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_prefix = os.path.join(save_dir, 'ckpt')\n",
    "check_point = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                  discriminator_optimizer=discriminator_optimizer,\n",
    "                                  generator=generator,\n",
    "                                  discriminator=discriminator)\n",
    "\n",
    "check_point.restore(tf.train.latest_checkpoint(save_dir))\n",
    "ckpts = glob.glob(save_dir+'/ckpt*.index')\n",
    "if ckpts:\n",
    "    print('Found checkpoints', ckpts)\n",
    "    #print([c.split('/')[-1].split('.')[0].split('-')[-1] for c in ckpts])\n",
    "    first_epoch = sorted([int(c.split('/')[-1].split('.')[0].split('-')[-1]) for c in ckpts])[-1] * CHECKPOINT_FREQ\n",
    "    print('Starting with epoch', first_epoch)\n",
    "else:\n",
    "    first_epoch = 0\n",
    "# TODO change starting epoch to correspond with checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uc5iJxWOOi9c"
   },
   "outputs": [],
   "source": [
    "# setup some parameters for training\n",
    "EPOCHS = 400\n",
    "noise_dim = 50 \n",
    "num_examples_to_generate = 16\n",
    "\n",
    "epochs_to_train = list(range(first_epoch, EPOCHS))\n",
    "\n",
    "# Seed is reused to animate a gif over time\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DQGa1XfO8bp"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_steps(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real = discriminator(images, training=True)\n",
    "        fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake)\n",
    "        disc_loss = discriminator_loss(real, fake)\n",
    "\n",
    "    grad_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    grad_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_gen, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_disc, discriminator.trainable_variables))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ymnFEUJRcNz"
   },
   "outputs": [],
   "source": [
    "image_dir = '%s/cat_images' % save_dir\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input, save=True):\n",
    "    predictions = model(test_input, training=False)\n",
    "    #print(predictions[0,...,0])\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow((np.array(predictions[i,...]+1, dtype=np.float32)/2))\n",
    "        plt.axis('off')\n",
    "    if not save:\n",
    "        print(tf.math.sigmoid(discriminator(predictions, training=False)))\n",
    "\n",
    "    if save:\n",
    "        plt.savefig('{}/image_at_epoch{:04d}.png'.format(image_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gREHOBebQVBF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in epochs:\n",
    "        start = time.time()\n",
    "\n",
    "        for i, image_batch in enumerate(dataset):\n",
    "            if i % 10 == 0:\n",
    "                print('Training on batch %d' % i)\n",
    "            if i == batch_per_epoch:\n",
    "                break\n",
    "            train_steps(tf.subtract(image_batch,-1)) # tf.subtract to make image range go from [-1, 1]\n",
    "        \n",
    "        # produce images for GIF\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch+1, seed)\n",
    "\n",
    "        if (epoch +1) % CHECKPOINT_FREQ == 0:\n",
    "          check_point.save(file_prefix = save_prefix)\n",
    "        \n",
    "\n",
    "        print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "generate_and_save_images(generator, EPOCHS, seed, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Will train following epochs:', epochs_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtMI6jwIRWqz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_images, epochs_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To make things more understandable, just going to run output through a sigmoid function\n",
    "print('Checking discriminator performance:')\n",
    "#print('When running on real cats:')\n",
    "#print(tf.math.sigmoid(discriminator(train_images[0], training=False)))\n",
    "print('When running on fake cats:')\n",
    "new_seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "generate_and_save_images(generator, EPOCHS, new_seed, save=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN-Sw0G2SsaX"
   },
   "outputs": [],
   "source": [
    "image_dir = './cat_training_checkpoints/cat_images'\n",
    "\n",
    "anim_file = '%s/cats.gif' % image_dir\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import PIL\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob(save_dir+'/cat_images/*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    # Have each image twice, otherwise gif moves too fast\n",
    "    writer.append_data(image)\n",
    "    writer.append_data(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynhzgOAPIao5"
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/tensorflow/docs\n",
    "\n",
    "#mport tensorflow_docs.vis.embed as embed\n",
    "#embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMD6DeWsJrKw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMfbA5bbRlKQYJrkZ+hdsNl",
   "collapsed_sections": [],
   "name": "Cat GAN Trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
